{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_PATH = \"../data/\" \n",
    "# DATA_PATH = \"/kaggle/input/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_PATH = os.path.join(DATA_PATH, \"nlp-ua-locations-extractions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see there two possible approaches to deal with the two-languages nature of the test data:\n",
    "- Train two separate models for the uk and ru NER, on the inference detect language of the sample first and apply correspondent model \n",
    "- Train multilingual model on the dataset which includes samples from both languages, with the same proportion as in the test data \n",
    "\n",
    "I will try the second approach first here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate languages ratio in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in /home/evgeniy/.local/bin/miniconda3/envs/iasa_nlp_env/lib/python3.9/site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in /home/evgeniy/.local/bin/miniconda3/envs/iasa_nlp_env/lib/python3.9/site-packages (from langid) (1.22.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/home/evgeniy/.local/bin/miniconda3/envs/iasa_nlp_env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import langid\n",
    "langid.set_languages(['ru', 'uk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(os.path.join(DATASETS_PATH, \"test.csv\"))\n",
    "df_test['lang'] = df_test['text'].apply(lambda x: langid.classify(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang\n",
       "uk    423\n",
       "ru     54\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.833333333333333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_ratio = (df_test['lang'] == 'uk').sum() / (df_test['lang'] == 'ru').sum()\n",
    "lang_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1010000, 5), (8028840, 6))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_geo_dataset = pd.read_csv(os.path.join(DATASETS_PATH, 'uk_geo_dataset.csv'), converters={\"loc_markers\": eval})\n",
    "ru_geo_dataset = pd.read_csv(os.path.join(DATASETS_PATH, 'ru_geo_dataset.csv'), converters={\"loc_markers\": eval})\n",
    "uk_geo_dataset.shape, ru_geo_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, uk_geo_dataset_light = train_test_split(uk_geo_dataset, stratify=uk_geo_dataset[\"is_valid\"], test_size=0.1, random_state=42)\n",
    "val_ratio = (uk_geo_dataset_light['is_valid'] == 1).sum() / uk_geo_dataset_light.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from different docs to increase diversity of the sub-dataset\n",
    "ru_geo_dataset_light = ru_geo_dataset.groupby('doc_id').first().sample(int(uk_geo_dataset_light.shape[0] / lang_ratio)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_geo_dataset_light['is_valid'] = ru_geo_dataset_light['text'].apply(lambda x: 1 if hash(x) % int(1/val_ratio) == 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>loc_markers</th>\n",
       "      <th>org_markers</th>\n",
       "      <th>per_markers</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>494552</th>\n",
       "      <td>Прокуратура не бере участі у виборчому процесі...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419794</th>\n",
       "      <td>”Особливо хочу відзначити наше недавнє досягне...</td>\n",
       "      <td>[(72, 90)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104648</th>\n",
       "      <td>Сергій Стаховський є лідером чоловічої збірної...</td>\n",
       "      <td>[(47, 54)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, 18)]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115911</th>\n",
       "      <td>Невже цифрові концтабори будують не лише на ум...</td>\n",
       "      <td>[(53, 58), (81, 87)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532374</th>\n",
       "      <td>Все лікування хлопчику проводили в Україні, ал...</td>\n",
       "      <td>[(35, 42), (88, 94)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "494552  Прокуратура не бере участі у виборчому процесі...   \n",
       "419794  ”Особливо хочу відзначити наше недавнє досягне...   \n",
       "104648  Сергій Стаховський є лідером чоловічої збірної...   \n",
       "115911  Невже цифрові концтабори будують не лише на ум...   \n",
       "532374  Все лікування хлопчику проводили в Україні, ал...   \n",
       "\n",
       "                 loc_markers org_markers per_markers  is_valid  \n",
       "494552                    []          []          []         0  \n",
       "419794            [(72, 90)]          []          []         0  \n",
       "104648            [(47, 54)]          []   [(0, 18)]         0  \n",
       "115911  [(53, 58), (81, 87)]          []          []         0  \n",
       "532374  [(35, 42), (88, 94)]          []          []         0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_geo_dataset_light.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_geo_dataset_light[['text', 'loc_markers', 'is_valid']].to_csv(os.path.join(DATASETS_PATH, \"uk_geo_dataset_light.csv\"), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_geo_dataset_light[['text', 'loc_markers', 'is_valid']].to_csv(os.path.join(DATASETS_PATH, \"ru_geo_dataset_light.csv\"), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_geo_dataset = pd.read_csv(os.path.join(DATASETS_PATH, \"uk_geo_dataset_light.csv\"), converters={'loc_markers': eval})\n",
    "ru_geo_dataset = pd.read_csv(os.path.join(DATASETS_PATH, \"ru_geo_dataset_light.csv\"), converters={'loc_markers': eval})\n",
    "\n",
    "uk_geo_dataset['lang'] = 'uk'\n",
    "ru_geo_dataset['lang'] = 'ru'\n",
    "df_locations = pd.concat([uk_geo_dataset, ru_geo_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evgeniy/.local/bin/miniconda3/envs/iasa_nlp_env/lib/python3.9/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/evgeniy/.local/bin/miniconda3/envs/iasa_nlp_env/lib/python3.9/site-packages/torch/cuda/__init__.py:529: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at ../c10/hip/HIPFunctions.cpp:110.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed23c5458503485889716d07d39c523a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113893 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping span /Ставропольский край/ expanded from Ставропольский край\n",
      "Skipping span вВолгограде expanded from Волгограде\n",
      "Skipping span вРоссии expanded from России\n",
      "Skipping span РФВладимир expanded from РФ\n",
      "Skipping span РФВладислав expanded from РФ\n",
      "Skipping span РФВладимира expanded from РФ\n",
      "Skipping span вМоскве expanded from Москве\n",
      "Skipping span ВЛондоне expanded from Лондоне\n",
      "Skipping span Спартак\"(Москва expanded from Москва\n",
      "Skipping span ВСША expanded from США\n",
      "Skipping span вБостоне expanded from Бостоне\n",
      "Skipping span Глостер(Gloucester expanded from Глостер\n",
      "Skipping span комитетРФ expanded from РФ\n",
      "Skipping span комитетРФ expanded from РФ\n",
      "Skipping span вРоссии expanded from России\n",
      "Skipping span ВКирове expanded from Кирове\n",
      "Skipping span вМоскву expanded from Москву\n",
      "Skipping span Израильcкие expanded from Израиль\n",
      "Skipping span РФЮлия expanded from РФ\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.training.iob_utils import biluo_to_iob, doc_to_biluo_tags\n",
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df_locations.loc_markers = df_locations.loc_markers.apply(lambda x: [[y[0], y[1], 'LOC']  for y in x])\n",
    "\n",
    "nlp = spacy.blank(\"xx\")\n",
    "\n",
    "def convert_to_conll(row):\n",
    "    data = {\n",
    "        \"text\": row['text'],\n",
    "        \"label\": row['loc_markers']\n",
    "    }\n",
    "    doc = nlp(data[\"text\"])\n",
    "    ents = []\n",
    "    for start, end, label in data[\"label\"]:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode='contract')\n",
    "        if span is None:\n",
    "            span_extended = doc.char_span(start, end, label=label, alignment_mode='expand')\n",
    "            print(f\"Skipping span {span_extended} expanded from {doc.text[start:end]}\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents\n",
    "    return {\n",
    "        'tokens': list([t.text for t in doc]),\n",
    "        'labels': list(biluo_to_iob(doc_to_biluo_tags(doc)))\n",
    "    }\n",
    "\n",
    "df_locations['conll'] = df_locations.progress_apply(convert_to_conll, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'O': 0, 'B-LOC': 1, 'I-LOC': 2}\n",
    "\n",
    "df_locations['tokens'] = df_locations.conll.str['tokens']\n",
    "df_locations['ner_tags'] = df_locations.conll.str['labels'].apply(lambda x: [label2id[t] for t in x])\n",
    "\n",
    "df_train = df_locations[df_locations.is_valid == 0]\n",
    "df_valid = df_locations[df_locations.is_valid == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['tokens', 'ner_tags']].to_json(\n",
    "    os.path.join(DATASETS_PATH, 'train_light.json'), orient='records', lines=True)\n",
    "df_valid[['tokens', 'ner_tags']].to_json(\n",
    "    os.path.join(DATASETS_PATH, 'valid_light.json'), orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5155ec736bbd4f81a8f43bfaf069e28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f754742cbe304da0802febc64dda7e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92afd7db54c64a4191347a7233352175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1c496dda8f460f8454195275d4021b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        'train': os.path.join(DATASETS_PATH, 'train_light.json'),\n",
    "        'val': os.path.join(DATASETS_PATH, 'valid_light.json'),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "label2id = {'O': 0, 'B-LOC': 1, 'I-LOC': 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    # 'youscan/ukr-roberta-base',\n",
    "    'bert-base-multilingual-cased',\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        elif word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            new_labels.append(labels[word_id])\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcacf1bc32d40a8a2b55c339cde070d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/112770 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a15316d1ad4c91848f7b359962311a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-ua-loc-ner\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evgeniy/.local/bin/miniconda3/envs/iasa_nlp_env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {'params': list(model.bert.parameters()), 'lr': 1e-5},\n",
    "    {'params': list(model.classifier.parameters()), 'lr': 1e-3}\n",
    "])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0.1*3*(tokenized_datasets['train'].num_rows/16),\n",
    "    num_training_steps=3*(tokenized_datasets['train'].num_rows/16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "label_names = list(label2id.keys())\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def comp_metrics(y_true: List[List[str]], y_pred: List[List[str]]):\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    tp, fp, fn = 0.0, 0.0, 0.0\n",
    "\n",
    "    for y_true_sample, y_pred_sample in zip(y_true, y_pred):\n",
    "        tp += len(set(y_true_sample) & set(y_pred_sample))\n",
    "        fp += len(set(y_pred_sample) - set(y_true_sample))\n",
    "        fn += len(set(y_true_sample) - set(y_pred_sample))\n",
    "    \n",
    "    precision = tp / (tp + fp) if tp + fp != 0 else 0.0 if tp + fn != 0.0 else 1.0\n",
    "    recall = tp / (tp + fn) if tp + fn != 0 else 1.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 112770\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 10\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1844386e8ebc40d6a66805fd1d2b0a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2a4fde48ab47dc84c7bbb98a5005cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-ua-loc-ner/checkpoint-1\n",
      "Configuration saved in bert-ua-loc-ner/checkpoint-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0088510513305664, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.5663265306122449, 'eval_runtime': 0.9954, 'eval_samples_per_second': 10.046, 'eval_steps_per_second': 1.005, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-ua-loc-ner/checkpoint-1/pytorch_model.bin\n",
      "tokenizer config file saved in bert-ua-loc-ner/checkpoint-1/tokenizer_config.json\n",
      "Special tokens file saved in bert-ua-loc-ner/checkpoint-1/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a96303363843a3b8f63a8c40cca22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-ua-loc-ner/checkpoint-2\n",
      "Configuration saved in bert-ua-loc-ner/checkpoint-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0086519718170166, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.5663265306122449, 'eval_runtime': 1.1357, 'eval_samples_per_second': 8.805, 'eval_steps_per_second': 0.881, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-ua-loc-ner/checkpoint-2/pytorch_model.bin\n",
      "tokenizer config file saved in bert-ua-loc-ner/checkpoint-2/tokenizer_config.json\n",
      "Special tokens file saved in bert-ua-loc-ner/checkpoint-2/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c2558ef0a84cae8e8478b080421505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-ua-loc-ner/checkpoint-3\n",
      "Configuration saved in bert-ua-loc-ner/checkpoint-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.008270502090454, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.5663265306122449, 'eval_runtime': 1.2271, 'eval_samples_per_second': 8.15, 'eval_steps_per_second': 0.815, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-ua-loc-ner/checkpoint-3/pytorch_model.bin\n",
      "tokenizer config file saved in bert-ua-loc-ner/checkpoint-3/tokenizer_config.json\n",
      "Special tokens file saved in bert-ua-loc-ner/checkpoint-3/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 39.5886, 'train_samples_per_second': 0.758, 'train_steps_per_second': 0.076, 'train_loss': 1.0679607391357422, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=1.0679607391357422, metrics={'train_runtime': 39.5886, 'train_samples_per_second': 0.758, 'train_steps_per_second': 0.076, 'train_loss': 1.0679607391357422, 'epoch': 3.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "train_sample = [tokenized_datasets['train'][i] for i in range(10)]\n",
    "val_sample = [tokenized_datasets['val'][i] for i in range(10)]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    # train_dataset=tokenized_datasets[\"train\"],\n",
    "    train_dataset=train_sample,\n",
    "    # eval_dataset=tokenized_datasets[\"val\"],\n",
    "    eval_dataset=val_sample,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file bert-ua-loc-ner/checkpoint-3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-ua-loc-ner/checkpoint-3/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-LOC\",\n",
      "    \"2\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 1,\n",
      "    \"I-LOC\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading configuration file bert-ua-loc-ner/checkpoint-3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-ua-loc-ner/checkpoint-3/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-LOC\",\n",
      "    \"2\": \"I-LOC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 1,\n",
      "    \"I-LOC\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file bert-ua-loc-ner/checkpoint-3/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at bert-ua-loc-ner/checkpoint-3/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "Didn't find file bert-ua-loc-ner/checkpoint-3/added_tokens.json. We won't load it.\n",
      "loading file bert-ua-loc-ner/checkpoint-3/vocab.txt\n",
      "loading file bert-ua-loc-ner/checkpoint-3/tokenizer.json\n",
      "loading file None\n",
      "loading file bert-ua-loc-ner/checkpoint-3/special_tokens_map.json\n",
      "loading file bert-ua-loc-ner/checkpoint-3/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"bert-ua-loc-ner/checkpoint-3/\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Всього таких полонених на підконтрольній угрупованню ДНР території кілька десятків.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_geo_dataset_light.loc[501309].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LOC',\n",
       "  'score': 0.4564191,\n",
       "  'word': 'пол',\n",
       "  'start': 13,\n",
       "  'end': 16},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.33921558,\n",
       "  'word': 'на',\n",
       "  'start': 23,\n",
       "  'end': 25},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.39106214,\n",
       "  'word': 'у',\n",
       "  'start': 41,\n",
       "  'end': 42},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.3782667,\n",
       "  'word': '##НР',\n",
       "  'start': 54,\n",
       "  'end': 56},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.4021538,\n",
       "  'word': 'території',\n",
       "  'start': 57,\n",
       "  'end': 66},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.43995804,\n",
       "  'word': '.',\n",
       "  'start': 82,\n",
       "  'end': 83}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classifier(uk_geo_dataset_light.loc[501309].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iasa_nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
